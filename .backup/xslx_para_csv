import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import spacy
import nltk
from nltk.corpus import stopwords
import re
import os
from collections import Counter
from unidecode import unidecode

# === 1. CONFIGURAÇÕES INICIAIS ===
ARQUIVO_EXCEL = "post_saudeReddit_COMPLETO.xlsx"
ABAS = ['positivo', 'negativo', 'neutro']

# Carregar NLP e Stopwords
try:
    nlp = spacy.load("pt_core_news_lg")
    print(" Modelo 'pt_core_news_lg' carregado.")
except:
    print(" Usando modelo 'pt_core_news_sm'.")
    nlp = spacy.load("pt_core_news_sm")

nltk.download('stopwords')
stop_words_custom = set(stopwords.words('portuguese')).union(nlp.Defaults.stop_words)

# Stopwords que queremos ignorar (conectivos e gírias)
stop_words_custom.update([
    "pra", "ta", "tipo", "ne", "ai", "dai", "q", "pq", "voce", "post", "galera",
    "ficar", "fiquei", "vou", "ter", "acho", "acha", "coisa", "pessoa", "ano"
])

# === 2. FUNÇÃO DE LIMPEZA DO TEXTO ===
def limpar_texto(texto):
    if not isinstance(texto, str): return ""
    # Remove links, menções e símbolos
    texto = re.sub(r"http\S+|www\S+|@\w+|[^A-Za-zÀ-ÖØ-öø-ÿ\s]", " ", texto)
    # Remove acentos e deixa em minúsculas
    texto = unidecode(texto).lower()
    doc = nlp(texto)
    # Mantém apenas substantivos e adjetivos para a nuvem
    return " ".join([t.lemma_.lower() for t in doc if t.pos_ in ["NOUN", "ADJ"] 
                     and t.lemma_.lower() not in stop_words_custom and len(t.text) > 2])

# === 3. PROCESSAMENTO E GERAÇÃO DE FICHEIROS ===
print(" A ler o ficheiro Excel e a processar abas...")

try:
    # Lê as abas e junta tudo
    lista_dfs = [pd.read_excel(ARQUIVO_EXCEL, sheet_name=aba) for aba in ABAS]
    df_total = pd.concat(lista_dfs, ignore_index=True)

    # LIMPEZA DOS ESPAÇOS: Transforma o texto enorme numa linha só
    df_total['texto'] = df_total['texto'].astype(str).str.replace(r'[\r\n]+', ' ', regex=True).str.strip()

    # Identifica todas as palavras-chave (amo, triste, terapia...)
    palavras_chave = df_total['palavraChave'].unique()

    for kw in palavras_chave:
        if pd.isna(kw): continue
        print(f" A processar palavra-chave: {kw}...")
        
        # Filtra apenas os dados desta palavra-chave
        df_kw = df_total[df_total['palavraChave'] == kw].copy()
        
        # 1. Salva o CSV formatado 
        df_kw_formatado = df_kw.rename(columns={
            'id':'ID', 'data':'TIMESTAMP', 'texto':'TEXTO', 
            'autor':'USUARIO', 'palavraChave':'EMOCAO', 'categoria':'SENTIMENTO'
        })
        nome_csv = f"{kw}_formatado.csv"
        df_kw_formatado[['ID', 'TIMESTAMP', 'TEXTO', 'USUARIO', 'EMOCAO', 'SENTIMENTO']].to_csv(nome_csv, index=False, quoting=1, encoding='utf-8')

        # 2. Prepara o texto para a Nuvem de Palavras
        texto_para_nuvem = " ".join([limpar_texto(t) for t in df_kw['texto']])
        
        if not texto_para_nuvem.strip(): 
            print(f" Sem palavras suficientes para {kw}")
            continue

        # 3. Gera e guarda a Nuvem de Palavras (PNG)
        wc = WordCloud(width=800, height=400, background_color='white', colormap='plasma', collocations=False).generate(texto_para_nuvem)
        plt.figure(figsize=(10, 5))
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title(f"Nuvem de Palavras: {kw.upper()}", fontsize=16)
        plt.savefig(f"nuvem_{kw}.png", bbox_inches='tight', dpi=300)
        plt.close()

        # 4. Gera e guarda a Tabela de Frequência (CSV)
        contagem = Counter(texto_para_nuvem.split())
        df_freq = pd.DataFrame(contagem.most_common(20), columns=["Palavra", "Frequência"])
        df_freq.to_csv(f"freq_{kw}.csv", index=False, encoding='utf-8')

    print("\n PROCESSO CONCLUÍDO COM SUCESSO!")

except Exception as e:
    print(f"Ocorreu um erro: {e}")
